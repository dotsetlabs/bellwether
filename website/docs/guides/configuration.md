---
title: Configuration
sidebar_position: 2
---

# Configuration

Bellwether uses a config-first approach where all settings are defined in `bellwether.yaml`.

## Getting Started

Create a configuration file with `bellwether init`:

```bash
bellwether init                    # Default structural mode (free, fast)
bellwether init --preset ci        # Optimized for CI/CD
bellwether init --preset security  # Security-focused testing
bellwether init --preset thorough  # Comprehensive testing
bellwether init --preset local     # Full mode with local Ollama
```

The generated file includes all options with helpful comments.

## Configuration File Location

Bellwether looks for configuration in this order:

1. `--config` flag (explicit path)
2. `./bellwether.yaml` (project root)
3. `./bellwether.json` (alternative format)

:::info Config Required
A `bellwether.yaml` config file is required for `bellwether test`. Run `bellwether init` to create one.
:::

## Full Configuration Reference

```yaml
# Bellwether Configuration
# Generated by: bellwether init
# Docs: https://docs.bellwether.sh/guides/configuration

# =============================================================================
# SERVER
# =============================================================================
# The MCP server to test. Required for bellwether test.
# Can also be passed as: bellwether test <command> [args...]

server:
  # Command to start the MCP server
  command: "npx @mcp/your-server"

  # Arguments to pass to the server command
  args: []

  # Timeout for server startup and tool calls (milliseconds)
  timeout: 30000

  # Additional environment variables for the server process
  # Use ${VAR} syntax for interpolation (see "Environment Variable Interpolation" below)
  # env:
  #   NODE_ENV: production
  #   DEBUG: "mcp:*"
  #   API_KEY: "${API_KEY}"
  #   LOG_LEVEL: "${LOG_LEVEL:-info}"

# =============================================================================
# TEST MODE
# =============================================================================
# structural: Fast, free, deterministic - compares tool schemas only (recommended)
# full: Uses LLM to generate intelligent test scenarios (requires API key)

mode: structural

# =============================================================================
# LLM SETTINGS
# =============================================================================
# Only used when mode is "full". Ignored in structural mode.

llm:
  # Provider: ollama (local, free), openai, or anthropic
  provider: ollama

  # Model to use. Leave empty for provider default.
  # Defaults: ollama=llama3.2, openai=gpt-4o-mini, anthropic=claude-haiku-4-5
  model: ""

  # Ollama settings (for local LLM)
  ollama:
    baseUrl: "http://localhost:11434"

  # API keys are loaded from environment variables or bellwether auth.
  # You can specify which env var to use:
  # openaiApiKeyEnvVar: OPENAI_API_KEY
  # anthropicApiKeyEnvVar: ANTHROPIC_API_KEY

  # SECURITY: Never put API keys directly in this file!
  # Use: bellwether auth  (interactive)
  # Or:  export OPENAI_API_KEY=sk-xxx

# =============================================================================
# TEST SETTINGS
# =============================================================================
# Only used when mode is "full". Ignored in structural mode.

test:
  # Personas provide different testing perspectives:
  # - technical_writer: Documentation quality, clarity, completeness
  # - security_tester: Security vulnerabilities, input validation
  # - qa_engineer: Edge cases, error handling, reliability
  # - novice_user: Usability, confusing behavior, missing guidance
  personas:
    - technical_writer

  # Maximum questions to ask per tool (1-10)
  # Lower = faster/cheaper, Higher = more thorough
  maxQuestionsPerTool: 3

  # Run personas in parallel for faster execution
  # Uses more memory but significantly reduces total time
  parallelPersonas: false

  # Skip error/edge case testing (faster but less thorough)
  skipErrorTests: false

# =============================================================================
# SCENARIOS
# =============================================================================
# Custom deterministic test scenarios (works in both modes).
# Define specific inputs and expected outputs for reproducible testing.

scenarios:
  # Path to scenarios YAML file
  # path: "./bellwether-tests.yaml"

  # Run ONLY scenarios (skip LLM-generated tests)
  # Useful for fast, deterministic CI checks
  only: false

# =============================================================================
# WORKFLOWS
# =============================================================================
# Multi-step workflow testing (requires mode: full).

workflows:
  # Path to workflow definitions YAML file
  # path: "./bellwether-workflows.yaml"

  # Enable LLM-based workflow discovery
  # Automatically generates workflows based on tool relationships
  discover: false

  # Track state changes between workflow steps
  trackState: false

# =============================================================================
# OUTPUT
# =============================================================================

output:
  # Directory for output files (AGENTS.md, baselines, reports)
  dir: "."

  # Output formats:
  # - agents.md: Human-readable markdown documentation
  # - json: Machine-readable JSON report
  # - both: Generate both formats
  format: both

  # Generate cloud-compatible baseline format
  # Required for bellwether upload
  cloudFormat: false

# =============================================================================
# BASELINE
# =============================================================================
# Baseline comparison for drift detection.

baseline:
  # Path to baseline file for comparison
  # comparePath: "./bellwether-baseline.json"

  # Fail if drift is detected (useful for CI)
  failOnDrift: false

  # Minimum confidence score (0-100) to report a change
  # Changes below this threshold are filtered out
  minConfidence: 0

  # Confidence threshold (0-100) for CI failure
  # Breaking changes below this may be LLM non-determinism
  confidenceThreshold: 80

# =============================================================================
# CACHE
# =============================================================================
# Response caching speeds up repeated runs.

cache:
  # Enable caching (recommended)
  enabled: true

  # Cache directory
  dir: ".bellwether/cache"

# =============================================================================
# LOGGING
# =============================================================================

logging:
  # Log level: debug, info, warn, error, silent
  level: info

  # Show verbose output (more details during execution)
  verbose: false
```

## Presets

Presets provide pre-configured settings for common use cases:

| Preset | Mode | Description |
|:-------|:-----|:------------|
| (default) | structural | Zero LLM, free, deterministic |
| `ci` | structural | Optimized for CI/CD, fails on drift |
| `security` | full | Security + technical personas, OpenAI |
| `thorough` | full | All 4 personas, workflow discovery |
| `local` | full | Local Ollama, free, private |

Use presets with `bellwether init`:

```bash
bellwether init --preset ci
```

## Provider Configuration

### Ollama (Local, Free)

```yaml
mode: full

llm:
  provider: ollama
  model: llama3.2
  ollama:
    baseUrl: "http://localhost:11434"
```

No API key required. Make sure Ollama is running locally.

### OpenAI

```yaml
mode: full

llm:
  provider: openai
  model: gpt-4o-mini  # or gpt-4o, gpt-4-turbo
```

Set API key via environment or `bellwether auth`:

```bash
export OPENAI_API_KEY=sk-xxx
# or
bellwether auth
```

### Anthropic

```yaml
mode: full

llm:
  provider: anthropic
  model: claude-haiku-4-5  # or claude-sonnet-4-5
```

Set API key via environment or `bellwether auth`:

```bash
export ANTHROPIC_API_KEY=sk-ant-xxx
# or
bellwether auth
```

## Mode Comparison

| Feature | Structural Mode | Full Mode |
|:--------|:----------------|:----------|
| Cost | Free | API costs apply |
| Speed | Fast (seconds) | Slower (minutes) |
| LLM Required | No | Yes |
| Deterministic | Yes | No |
| Tool Schema Testing | Yes | Yes |
| Behavioral Testing | No | Yes |
| Security Analysis | No | Yes |
| AGENTS.md Quality | Basic | Comprehensive |

### When to Use Each Mode

**Structural Mode (Default)**
- CI/CD pipelines
- PR checks
- Automated testing
- When you need deterministic results
- When you want to avoid API costs

**Full Mode**
- Local development
- Deep exploration of server capabilities
- Security audits
- Generating comprehensive documentation
- Multi-persona testing

## Environment Variables

| Variable | Description |
|:---------|:------------|
| `OPENAI_API_KEY` | OpenAI API key |
| `ANTHROPIC_API_KEY` | Anthropic API key |
| `OLLAMA_BASE_URL` | Ollama server URL (default: `http://localhost:11434`) |
| `LOG_LEVEL` | Override logging level |
| `BELLWETHER_SESSION` | Cloud authentication token |

## Environment Variable Interpolation

Bellwether supports environment variable interpolation in your configuration file, allowing you to reference secrets without committing them to version control.

### Syntax

```yaml
server:
  env:
    # Basic interpolation - pulls from shell or .env file
    API_KEY: "${API_KEY}"
    SERVICE_URL: "${SERVICE_URL}"

    # With default values - uses fallback if var is not set
    LOG_LEVEL: "${LOG_LEVEL:-info}"
    DEBUG: "${DEBUG:-false}"
    TIMEOUT: "${TIMEOUT:-30000}"
```

### How It Works

1. **At runtime**, Bellwether replaces `${VAR}` with the value of the environment variable `VAR`
2. **Default values** can be specified with `${VAR:-default}` syntax
3. **Unset variables** without defaults are left as-is (useful for catching missing config)

### Example Workflow

```bash
# Set environment variables
export PLEX_URL="http://192.168.1.100:32400"
export PLEX_TOKEN="your-token-here"

# Or use a .env file with dotenv
# PLEX_URL=http://192.168.1.100:32400
# PLEX_TOKEN=your-token-here

# Run bellwether - it will interpolate the values
bellwether test
```

:::tip Commit-Safe Configuration
This pattern allows you to commit `bellwether.yaml` to version control while keeping secrets in environment variables or `.env` files (which should be gitignored).
:::

### Common Patterns

**API Keys and Tokens:**
```yaml
server:
  env:
    API_KEY: "${API_KEY}"
    AUTH_TOKEN: "${AUTH_TOKEN}"
```

**URLs with Defaults:**
```yaml
server:
  env:
    BASE_URL: "${BASE_URL:-http://localhost:3000}"
    API_ENDPOINT: "${API_ENDPOINT:-/api/v1}"
```

**Feature Flags:**
```yaml
server:
  env:
    DEBUG: "${DEBUG:-false}"
    LOG_LEVEL: "${LOG_LEVEL:-info}"
```

## Multiple Configurations

Manage different configs for different environments:

```bash
# Development config
bellwether init --preset local
mv bellwether.yaml configs/dev.yaml

# CI config
bellwether init --preset ci
mv bellwether.yaml configs/ci.yaml

# Use specific config
bellwether test --config configs/ci.yaml npx your-server
```

## Configuration Validation

Bellwether validates your config on startup and shows helpful errors:

```
Invalid configuration:
  - mode: Must be "structural" or "full"
  - llm.provider: Must be one of: ollama, openai, anthropic
  - test.maxQuestionsPerTool: Must be between 1 and 10
```

## Best Practices

1. **Version control your config** - Commit `bellwether.yaml` to your repo
2. **Use structural mode for CI** - Deterministic, free, fast
3. **Never commit API keys** - Use environment variables or `bellwether auth`
4. **Use presets as starting points** - Customize from there
5. **Enable JSON output for baselines** - Set `output.format: both` or `json`

## See Also

- [init](/cli/init) - Generate configuration
- [test](/cli/test) - Run tests using configuration
- [baseline](/cli/baseline) - Manage baselines
- [Custom Scenarios](/guides/custom-scenarios) - YAML-defined test cases
- [CI/CD Integration](/guides/ci-cd) - Pipeline configurations
