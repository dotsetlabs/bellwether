/**
 * Configuration templates for bellwether init.
 *
 * These templates include ALL possible options with comments,
 * making the config file self-documenting.
 */

/**
 * Configuration template options.
 */
export interface ConfigTemplateOptions {
  /** Server command (optional, can be set later) */
  serverCommand?: string;
  /** Server arguments */
  serverArgs?: string[];
  /** LLM provider for explore command */
  provider?: 'ollama' | 'openai' | 'anthropic';
  /** Preset name used to generate this config */
  preset?: string;
  /** Environment variables detected from .env.example */
  envVars?: string[];
  /** Whether to optimize for CI (affects baseline.failOnDrift default) */
  ciOptimized?: boolean;
}

/**
 * Generate a comprehensive bellwether.yaml configuration file.
 *
 * The generated file includes ALL possible options with explanatory comments,
 * so users can customize without consulting documentation.
 *
 * This config is used by both 'bellwether check' and 'bellwether explore' commands.
 */
export function generateConfigTemplate(options: ConfigTemplateOptions = {}): string {
  const {
    serverCommand = '',
    serverArgs = [],
    provider = 'ollama',
    preset,
    envVars = [],
    ciOptimized = false,
  } = options;

  const serverArgsYaml = serverArgs.length > 0
    ? `\n  args:\n${serverArgs.map(arg => `    - "${arg}"`).join('\n')}`
    : '\n  args: []';

  const presetComment = preset ? `# Generated with: bellwether init --preset ${preset}\n` : '';

  // Generate env section if env vars were detected
  const envVarsYaml = envVars.length > 0
    ? `\n  env:\n${envVars.map(v => `    ${v}: "\${${v}}"`).join('\n')}`
    : '';

  return `# Bellwether Configuration
# Generated by: bellwether init
# Docs: https://docs.bellwether.sh/guides/configuration
${presetComment}
# This config is used by both commands:
#   bellwether check   - Schema validation and drift detection (free, fast)
#   bellwether explore - LLM-powered behavioral exploration

# =============================================================================
# SERVER (used by both commands)
# =============================================================================
# The MCP server to validate/explore.

server:
  # Command to start the MCP server
  command: "${serverCommand}"
  # Arguments to pass to the server command${serverArgsYaml}

  # Timeout for server startup and tool calls (milliseconds, default: 30000)
  timeout: 30000

  # Additional environment variables for the server process
  # Use \${VAR} syntax to reference environment variables${envVarsYaml}${envVars.length === 0 ? `
  # env:
  #   NODE_ENV: production
  #   API_KEY: "\${API_KEY}"` : ''}

# =============================================================================
# SCENARIOS (used by both commands)
# =============================================================================
# Custom deterministic test scenarios for reproducible testing.

scenarios:
  # Path to scenarios YAML file
  # path: "./bellwether-tests.yaml"

  # Run ONLY scenarios (skip generated tests)
  only: false

# =============================================================================
# OUTPUT (used by both commands)
# =============================================================================
# bellwether check outputs: CONTRACT.md, bellwether-check.json
# bellwether explore outputs: AGENTS.md, bellwether-explore.json

output:
  # Directory for output files
  dir: "."

# =============================================================================
# CHECK COMMAND SETTINGS
# =============================================================================
# Settings for 'bellwether check' - schema validation and drift detection.
# Free, fast, deterministic. No LLM required.

baseline:
  # Path to baseline file for comparison
  # comparePath: "./bellwether-baseline.json"

  # Fail if drift is detected (useful for CI)
  failOnDrift: ${ciOptimized ? 'true' : 'false'}

  # Minimum confidence score (0-100) to report a change
  minConfidence: 0

  # Confidence threshold (0-100) for CI failure
  confidenceThreshold: 80

# =============================================================================
# EXPLORE COMMAND SETTINGS
# =============================================================================
# Settings for 'bellwether explore' - LLM-powered behavioral exploration.
# Requires LLM provider (Ollama is free and local).

llm:
  # Provider: ollama (local, free), openai, or anthropic
  provider: ${provider}

  # Model to use. Leave empty for provider default.
  # Defaults: ollama=llama3.2, openai=gpt-4o-mini, anthropic=claude-haiku-4-5
  model: ""

  # Ollama settings (for local LLM)
  ollama:
    baseUrl: "http://localhost:11434"

  # API keys are loaded from environment variables or 'bellwether auth'.
  # SECURITY: Never put API keys directly in this file!

explore:
  # Personas provide different testing perspectives:
  # - technical_writer: Documentation quality, clarity, completeness
  # - security_tester: Security vulnerabilities, input validation
  # - qa_engineer: Edge cases, error handling, reliability
  # - novice_user: Usability, confusing behavior, missing guidance
  personas:
    - technical_writer

  # Maximum questions to ask per tool (1-10)
  maxQuestionsPerTool: 3

  # Run personas in parallel for faster execution
  parallelPersonas: false

  # Skip error/edge case testing
  skipErrorTests: false

workflows:
  # Path to workflow definitions YAML file
  # path: "./bellwether-workflows.yaml"

  # Enable LLM-based workflow discovery
  discover: false

  # Track state changes between workflow steps
  trackState: false

# =============================================================================
# COMMON SETTINGS
# =============================================================================

cache:
  # Enable response caching (recommended)
  enabled: true

  # Cache directory
  dir: ".bellwether/cache"

logging:
  # Log level: debug, info, warn, error, silent
  level: info

  # Show verbose output
  verbose: false
`;
}

/**
 * Preset configurations for common use cases.
 */
export const PRESETS: Record<string, ConfigTemplateOptions> = {
  /**
   * CI preset: Optimized for 'bellwether check' in CI/CD pipelines.
   * Fast, free, deterministic with failOnDrift enabled.
   */
  ci: {
    ciOptimized: true,
    provider: 'ollama', // Not used by check, but sensible default
  },

  /**
   * Security preset: Optimized for 'bellwether explore' with security focus.
   */
  security: {
    provider: 'openai',
  },

  /**
   * Thorough preset: Optimized for 'bellwether explore' with all personas.
   */
  thorough: {
    provider: 'openai',
  },

  /**
   * Local preset: Optimized for 'bellwether explore' with local Ollama.
   * Free, private, no API keys needed.
   */
  local: {
    provider: 'ollama',
  },
};

/**
 * Get preset-specific YAML overrides for documentation purposes.
 *
 * @param presetName - Name of the preset (ci, security, thorough, local)
 * @returns YAML string with preset-specific settings or null if preset not found
 */
export function getPresetOverrides(presetName: string): string | null {
  const preset = PRESETS[presetName];
  if (!preset) return null;

  // Return additional YAML to append/override based on preset
  switch (presetName) {
    case 'ci':
      return `
# CI Preset Overrides
# Optimized for 'bellwether check' in CI/CD pipelines

baseline:
  failOnDrift: true

logging:
  level: warn
`;

    case 'security':
      return `
# Security Preset Overrides
# Optimized for 'bellwether explore' with security focus

llm:
  provider: openai
  model: gpt-4o  # Better reasoning for security analysis

explore:
  personas:
    - technical_writer
    - security_tester
    - qa_engineer
  maxQuestionsPerTool: 5
  skipErrorTests: false
`;

    case 'thorough':
      return `
# Thorough Preset Overrides
# Comprehensive exploration with all personas

llm:
  provider: openai
  model: gpt-4o

explore:
  personas:
    - technical_writer
    - security_tester
    - qa_engineer
    - novice_user
  maxQuestionsPerTool: 5
  parallelPersonas: true
  skipErrorTests: false

workflows:
  discover: true
`;

    case 'local':
      return `
# Local Preset Overrides
# Exploration using local Ollama (free, private)

llm:
  provider: ollama
  model: llama3.2
  ollama:
    baseUrl: "http://localhost:11434"

explore:
  personas:
    - technical_writer
  maxQuestionsPerTool: 3
`;

    default:
      return null;
  }
}

/**
 * Generate a preset-specific configuration.
 */
export function generatePresetConfig(presetName: string, options: ConfigTemplateOptions = {}): string {
  const preset = PRESETS[presetName];
  if (!preset) {
    throw new Error(`Unknown preset: ${presetName}. Available: ${Object.keys(PRESETS).join(', ')}`);
  }

  return generateConfigTemplate({
    ...preset,
    ...options,
    preset: presetName,
  });
}
