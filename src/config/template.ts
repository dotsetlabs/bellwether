/**
 * Configuration templates for bellwether init.
 *
 * These templates include ALL possible options with comments,
 * making the config file self-documenting.
 */

/**
 * Configuration template options.
 */
export interface ConfigTemplateOptions {
  /** Server command (optional, can be set later) */
  serverCommand?: string;
  /** Server arguments */
  serverArgs?: string[];
  /** Test mode: structural (free) or full (LLM) */
  mode?: 'structural' | 'full';
  /** LLM provider for full mode */
  provider?: 'ollama' | 'openai' | 'anthropic';
  /** Preset name used to generate this config */
  preset?: string;
}

/**
 * Generate a comprehensive bellwether.yaml configuration file.
 *
 * The generated file includes ALL possible options with explanatory comments,
 * so users can customize without consulting documentation.
 */
export function generateConfigTemplate(options: ConfigTemplateOptions = {}): string {
  const {
    serverCommand = '',
    serverArgs = [],
    mode = 'structural',
    provider = 'ollama',
    preset,
  } = options;

  const serverArgsYaml = serverArgs.length > 0
    ? `\n  args:\n${serverArgs.map(arg => `    - "${arg}"`).join('\n')}`
    : '\n  args: []';

  const presetComment = preset ? `# Generated with: bellwether init --preset ${preset}\n` : '';

  // For structural mode, we comment out most LLM and test settings
  // For full mode, we enable them with sensible defaults
  const isStructural = mode === 'structural';

  return `# Bellwether Configuration
# Generated by: bellwether init
# Docs: https://docs.bellwether.sh/guides/configuration
${presetComment}
# =============================================================================
# SERVER
# =============================================================================
# The MCP server to test. Required for bellwether test.
# Can also be passed as: bellwether test <command> [args...]

server:
  # Command to start the MCP server
  command: "${serverCommand}"
  # Arguments to pass to the server command${serverArgsYaml}

  # Timeout for server startup and tool calls (milliseconds)
  timeout: 30000

  # Additional environment variables for the server process
  # env:
  #   NODE_ENV: production
  #   DEBUG: "mcp:*"

# =============================================================================
# TEST MODE
# =============================================================================
# structural: Fast, free, deterministic - compares tool schemas only (recommended)
# full: Uses LLM to generate intelligent test scenarios (requires API key)

mode: ${mode}

# =============================================================================
# LLM SETTINGS
# =============================================================================
# Only used when mode is "full". Ignored in structural mode.

llm:
  # Provider: ollama (local, free), openai, or anthropic
  provider: ${provider}

  # Model to use. Leave empty for provider default.
  # Defaults: ollama=llama3.2, openai=gpt-4o-mini, anthropic=claude-haiku-4-5
  model: ""

  # Ollama settings (for local LLM)
  ollama:
    baseUrl: "http://localhost:11434"

  # API keys are loaded from environment variables or bellwether auth.
  # You can specify which env var to use:
  # openaiApiKeyEnvVar: OPENAI_API_KEY
  # anthropicApiKeyEnvVar: ANTHROPIC_API_KEY

  # SECURITY: Never put API keys directly in this file!
  # Use: bellwether auth  (interactive)
  # Or:  export OPENAI_API_KEY=sk-xxx

# =============================================================================
# TEST SETTINGS
# =============================================================================
# Only used when mode is "full". Ignored in structural mode.

test:
  # Personas provide different testing perspectives:
  # - technical_writer: Documentation quality, clarity, completeness
  # - security_tester: Security vulnerabilities, input validation
  # - qa_engineer: Edge cases, error handling, reliability
  # - novice_user: Usability, confusing behavior, missing guidance
  personas: ${isStructural ? '[]  # Add personas for full mode' : '\n    - technical_writer'}

  # Maximum questions to ask per tool (1-10)
  # Lower = faster/cheaper, Higher = more thorough
  maxQuestionsPerTool: 3

  # Run personas in parallel for faster execution
  # Uses more memory but significantly reduces total time
  parallelPersonas: false

  # Skip error/edge case testing (faster but less thorough)
  skipErrorTests: false

# =============================================================================
# SCENARIOS
# =============================================================================
# Custom deterministic test scenarios (works in both modes).
# Define specific inputs and expected outputs for reproducible testing.

scenarios:
  # Path to scenarios YAML file
  # path: "./bellwether-tests.yaml"

  # Run ONLY scenarios (skip LLM-generated tests)
  # Useful for fast, deterministic CI checks
  only: false

# =============================================================================
# WORKFLOWS
# =============================================================================
# Multi-step workflow testing (requires mode: full).

workflows:
  # Path to workflow definitions YAML file
  # path: "./bellwether-workflows.yaml"

  # Enable LLM-based workflow discovery
  # Automatically generates workflows based on tool relationships
  discover: false

  # Track state changes between workflow steps
  trackState: false

# =============================================================================
# OUTPUT
# =============================================================================

output:
  # Directory for output files (AGENTS.md, baselines, reports)
  dir: "."

  # Output formats:
  # - agents.md: Human-readable markdown documentation
  # - json: Machine-readable JSON report
  # - both: Generate both formats
  format: agents.md

  # Generate cloud-compatible baseline format
  # Required for bellwether upload
  cloudFormat: false

# =============================================================================
# BASELINE
# =============================================================================
# Baseline comparison for drift detection.

baseline:
  # Path to baseline file for comparison
  # comparePath: "./bellwether-baseline.json"

  # Fail if drift is detected (useful for CI)
  failOnDrift: ${isStructural ? 'true' : 'false'}

  # Minimum confidence score (0-100) to report a change
  # Changes below this threshold are filtered out
  minConfidence: 0

  # Confidence threshold (0-100) for CI failure
  # Breaking changes below this may be LLM non-determinism
  confidenceThreshold: 80

# =============================================================================
# CACHE
# =============================================================================
# Response caching speeds up repeated runs.

cache:
  # Enable caching (recommended)
  enabled: true

  # Cache directory
  dir: ".bellwether/cache"

# =============================================================================
# LOGGING
# =============================================================================

logging:
  # Log level: debug, info, warn, error, silent
  level: info

  # Show verbose output (more details during execution)
  verbose: false
`;
}

/**
 * Preset configurations for common use cases.
 */
export const PRESETS: Record<string, ConfigTemplateOptions> = {
  /**
   * CI preset: Fast, free, deterministic structural testing.
   * Perfect for PR checks and automated pipelines.
   */
  ci: {
    mode: 'structural',
    provider: 'ollama', // Not used in structural mode
  },

  /**
   * Security preset: Multi-persona testing with security focus.
   * Uses LLM for intelligent security-focused test generation.
   */
  security: {
    mode: 'full',
    provider: 'openai',
  },

  /**
   * Thorough preset: Comprehensive testing with all personas.
   * Best for release testing and deep exploration.
   */
  thorough: {
    mode: 'full',
    provider: 'openai',
  },

  /**
   * Local preset: Full LLM testing using local Ollama.
   * Free, private, no API keys needed.
   */
  local: {
    mode: 'full',
    provider: 'ollama',
  },
};

/**
 * Get preset-specific overrides for the config template.
 */
export function getPresetOverrides(presetName: string): Partial<string> | null {
  const preset = PRESETS[presetName];
  if (!preset) return null;

  // Return additional YAML to append/override based on preset
  switch (presetName) {
    case 'ci':
      return `
# CI Preset Overrides
# Optimized for fast, free, deterministic CI/CD checks

mode: structural

baseline:
  failOnDrift: true

logging:
  level: warn
`;

    case 'security':
      return `
# Security Preset Overrides
# Focused on security testing with multiple personas

mode: full

llm:
  provider: openai
  model: gpt-4o  # Better reasoning for security analysis

test:
  personas:
    - technical_writer
    - security_tester
    - qa_engineer
  maxQuestionsPerTool: 5
  skipErrorTests: false
`;

    case 'thorough':
      return `
# Thorough Preset Overrides
# Comprehensive testing with all personas

mode: full

llm:
  provider: openai
  model: gpt-4o

test:
  personas:
    - technical_writer
    - security_tester
    - qa_engineer
    - novice_user
  maxQuestionsPerTool: 5
  parallelPersonas: true
  skipErrorTests: false

workflows:
  discover: true
`;

    case 'local':
      return `
# Local Preset Overrides
# Full LLM testing using local Ollama (free, private)

mode: full

llm:
  provider: ollama
  model: llama3.2
  ollama:
    baseUrl: "http://localhost:11434"

test:
  personas:
    - technical_writer
  maxQuestionsPerTool: 3
`;

    default:
      return null;
  }
}

/**
 * Generate a preset-specific configuration.
 */
export function generatePresetConfig(presetName: string, options: ConfigTemplateOptions = {}): string {
  const preset = PRESETS[presetName];
  if (!preset) {
    throw new Error(`Unknown preset: ${presetName}. Available: ${Object.keys(PRESETS).join(', ')}`);
  }

  return generateConfigTemplate({
    ...preset,
    ...options,
    preset: presetName,
  });
}
