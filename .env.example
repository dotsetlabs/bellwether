# Bellwether CLI Configuration
# Copy this file to .env and fill in your values
#
# For users: Only the LLM provider section is required (and only for explore mode)
# Check mode (default) requires no API keys.

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# Required for explore mode testing. Check mode (default) requires no API keys.
# Choose at least one provider if using explore mode.

# OpenAI - recommended for best interview results
# Get your key at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-key-here

# Anthropic Claude - excellent alternative with strong reasoning
# Get your key at: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Ollama - free, runs locally (no API key needed)
# Install from: https://ollama.ai
# Default URL is http://localhost:11434, only set if using non-standard port
# OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# DEVELOPMENT (for contributors)
# =============================================================================

# --- LLM Testing ---
# For testing LLM integrations, you'll need at least one provider key.
# We recommend starting with Ollama for free local testing:
#   1. Install Ollama: https://ollama.ai
#   2. Run: ollama serve
#   3. Pull the default model: ollama pull qwen3:8b
# No API key needed for Ollama.

# --- Running Tests ---
# Unit tests don't require any environment variables.
# Integration tests may need LLM provider keys.
#   npm test           # Run all tests
#   npm run test:watch # Watch mode

# --- Debug Output ---
# The CLI uses pino for logging. Set log level in bellwether.yaml:
#   logging:
#     level: debug  # debug, info, warn, error, silent
